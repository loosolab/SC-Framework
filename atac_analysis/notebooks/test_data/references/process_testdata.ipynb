{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7114c59",
   "metadata": {},
   "source": [
    "# Prepare scATAC Testdata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db6c2b",
   "metadata": {},
   "source": [
    "This notebook was used to pre-process and subset the ATAC-seq test data. The raw files were downloaded from 10X Datasets. More information can be found in the metadata HTML files [Metadata 10x PBMC v2](10k_pbmc_ATACv2_nextgem_Chromium_Controller_web_summary.html), [Metadata 10x PBMC v1.1](10k_pbmc_ATACv1p1_nextgem_Chromium_X_web_summary.html), which are provided within this directory.\n",
    "\n",
    "Briefly, both datasets are merged at matrix and BAM file levels, and the user defines the regions and number of cells to keep. At matrix level, the peak/cell matrix provided by 10x is converted to bins, which allows for better overlap of features between the datasets. The notebook will then subset the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6b431",
   "metadata": {},
   "source": [
    "## Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy as scp\n",
    "import numpy as np\n",
    "import glob\n",
    "import pysam\n",
    "import anndata as ad\n",
    "\n",
    "import pyranges as pr\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "import sctoolbox\n",
    "import sctoolbox.utils as utils\n",
    "import sctoolbox.tools as tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d86d5",
   "metadata": {},
   "source": [
    "## General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfcd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mtx = \"/mnt/workspace2/jdetlef/experimental/inspect_testdata/10k_PBMC-selection/\"\n",
    "\n",
    "header = None\n",
    "barcode_index = 0\n",
    "genes_index = 0\n",
    "delimiter = \"\\t\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375aacd7",
   "metadata": {},
   "source": [
    "## Make unique index from bedfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121bf33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks_to_bins(adata, chromsizes, bin_size=5000):\n",
    "\n",
    "    # 1) Prepare your peaks DataFrame with an explicit index\n",
    "    adata.var[\"peak_idx\"] = np.arange(adata.var.shape[0])\n",
    "\n",
    "    chrom_lengths = pd.read_csv(chromsizes, sep='\\t')\n",
    "    chrom_lengths.set_index('id', inplace=True)\n",
    "    chrom_lengths.pop('length.1')\n",
    "    chrom_lengths = chrom_lengths.to_dict()['length']\n",
    "\n",
    "    bins = []\n",
    "    for chrom, length in chrom_lengths.items():\n",
    "        for start in range(0, length, bin_size):\n",
    "            bins.append((chrom, start, min(start+bin_size, length)))\n",
    "    bins_df = pd.DataFrame(bins, columns=[\"Chromosome\",\"Start\",\"End\"])\n",
    "    bins_df[\"bin_idx\"] = np.arange(bins_df.shape[0])\n",
    "\n",
    "    # 2) Turn into PyRanges (include the index columns)\n",
    "    gr_peaks = pr.PyRanges(adata.var)\n",
    "    gr_bins  = pr.PyRanges(bins_df)\n",
    "\n",
    "    # 3) Join to get bin↔peak overlaps (will carry bin_idx & peak_idx)\n",
    "    overlap = gr_bins.join(gr_peaks).df\n",
    "    # overlap columns include: Chromosome, Start, End, bin_idx, Start_b, End_b, peak_idx\n",
    "\n",
    "    # 4) Build the sparse “peak→bin” matrix M\n",
    "    rows = overlap[\"peak_idx\"].values\n",
    "    cols = overlap[\"bin_idx\"].values\n",
    "    data = np.ones_like(rows, dtype=np.int8)\n",
    "    M = coo_matrix((data, (rows, cols)),\n",
    "                   shape=(adata.n_vars, bins_df.shape[0])).tocsr()\n",
    "\n",
    "    # 5) Multiply to get cell×bin counts\n",
    "    X_bins = adata.X.dot(M)  # sparse CSR result\n",
    "\n",
    "    # 6) Wrap into a new AnnData and save\n",
    "    var_bins = bins_df.set_index(\"bin_idx\")[[\"Chromosome\",\"Start\",\"End\"]]\n",
    "    var_bins.index = [f\"{c}:{s}-{e}\" for c,s,e in var_bins[[\"Chromosome\",\"Start\",\"End\"]].itertuples(index=False)]\n",
    "    adata_bins = ad.AnnData(X=X_bins, obs=adata.obs.copy(), var=var_bins)\n",
    "    adata = adata_bins\n",
    "\n",
    "    return adata\n",
    "\n",
    "\n",
    "def make_unique_index(path):\n",
    "    subdirs = next(os.walk(path_mtx))[1]\n",
    "    for directory in subdirs:\n",
    "        subdir = os.path.join(path, directory)\n",
    "        peaks_bed = os.path.join(subdir, 'peaks.bed')\n",
    "        output = os.path.join(subdir, 'peaks.tsv')\n",
    "        peaks_from_bed = pd.read_csv(peaks_bed, header=header, delimiter=delimiter)\n",
    "\n",
    "        custom_index = []\n",
    "\n",
    "        for row in peaks_from_bed.iterrows():\n",
    "            custom_index.append(f'{row[1][0]}:{row[1][1]}-{row[1][2]}')\n",
    "\n",
    "        peaks_from_bed['index'] = custom_index\n",
    "        peaks_from_bed.set_index('index', inplace=True)\n",
    "        peaks_from_bed.to_csv(output, header=False, sep='\\t')\n",
    "        \n",
    "# make_unique_index(path_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc933e",
   "metadata": {},
   "source": [
    "## Input filenames adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c31a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust in case of different naming schemes for any of the input files\n",
    "\n",
    "mtx = '*matrix.mtx*'  # pattern for the file that contains counts\n",
    "barcodes = '*barcodes.tsv*'  # pattern for the file that contains barcode information\n",
    "variables = '*peaks.tsv*'  # pattern for the optional file that contains variable information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ff61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load anndata object\n",
    "adata = utils.assemblers.from_mtx(path_mtx, mtx=mtx, barcodes=barcodes, variables=variables, variables_index=genes_index, header=None, var_error=False)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96843698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename var columns\n",
    "adata.var.rename(columns={'1':'Chromosome', '2':'Start', '3':'End'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert peaks to bins to join datasets\n",
    "adata = peaks_to_bins(adata, chromsizes='/mnt/flatfiles/organisms/new_organism/homo_sapiens/109/homo_sapiens.109.chrom.sizes', bin_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6841a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename barcodes to rule out duplicates between samples\n",
    "mapping = {}\n",
    "batched_barcodes = []\n",
    "for row in adata.obs.iterrows():\n",
    "    barcode = row[0]\n",
    "    batch = row[1]['batch']\n",
    "    barcode_raw = barcode.split('-')[0]\n",
    "    batched_barcodes.append(barcode_raw + '-' + batch)\n",
    "    \n",
    "    if row[1][1] not in mapping:\n",
    "        mapping[row[1][1]] = batch\n",
    "    \n",
    "adata.obs['batched_barcodes'] = batched_barcodes\n",
    "adata.obs.set_index('batched_barcodes', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50beee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add batched barcodes to the bamfiles\n",
    "def tag_batch(in_bam, out_bam, batch_id):\n",
    "    bam_in  = pysam.AlignmentFile(in_bam,  \"rb\")\n",
    "    bam_out = pysam.AlignmentFile(out_bam, \"wb\", template=bam_in)\n",
    "    for read in bam_in.fetch(until_eof=True):\n",
    "        if read.has_tag(\"CB\"):\n",
    "            old_cb = read.get_tag(\"CB\")\n",
    "            raw_bc = old_cb.split('-')[0]\n",
    "            new_cb = f\"{raw_bc}-{batch_id}\"\n",
    "            read.set_tag(\"CB\", new_cb)  # pysam will infer the Z type\n",
    "        bam_out.write(read)\n",
    "    bam_in.close()\n",
    "    bam_out.close()\n",
    "\n",
    "## Apply to both samples\n",
    "#bamfile_path = \"/mnt/workspace2/jdetlef/experimental/inspect_testdata/10k_PBMC-selection/bamfiles\"\n",
    "#print(f'adding batch information to {bamfile_path}')\n",
    "#tag_batch(os.path.join(bamfile_path, \"10k_pbmc_ATACv1p1_nextgem_Chromium_X_possorted_bam.bam\"), os.path.join(bamfile_path,\"10k_pbmc_ATACv1p1_nextgem_Chromium_X_batched_bam.bam\"), 1)\n",
    "#print(\"10k_pbmc_ATACv1p1_nextgem_Chromium_X_possorted_bam.bam DONE\")\n",
    "#tag_batch(os.path.join(bamfile_path, \"10k_pbmc_ATACv2_nextgem_Chromium_Controller_possorted_bam.bam\"), os.path.join(bamfile_path, \"10k_pbmc_ATACv2_nextgem_Chromium_Controller_batched_bam.bam\"), 0)\n",
    "#print(\"10k_pbmc_ATACv2_nextgem_Chromium_Controller_possorted_bam.bam DONE\")\n",
    "\n",
    "## NOW MERGE WITH SAMTOOLS --> COMMAND LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4fe361",
   "metadata": {},
   "source": [
    "## Subset adata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset n_cells randomly selected\n",
    "# Subset by chromosomes\n",
    "n_cells = 80000\n",
    "chroms_to_keep= ['chr1', 'chr2']\n",
    "\n",
    "# ,'chr10', 'chr11', 'chr12','chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr1', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY'\n",
    "\n",
    "barcode_sample = np.random.choice(adata.obs.index, size=n_cells)\n",
    "\n",
    "obs_mask = adata.obs.index.isin(barcode_sample)\n",
    "var_mask = adata.var['Chromosome'].isin(chroms_to_keep)\n",
    "\n",
    "subdata = adata[obs_mask, var_mask]\n",
    "\n",
    "subdata = subdata[np.sum((subdata.X > 0), axis=1) > 100]\n",
    "subdata = subdata[:, subdata.X.sum(axis=0) > 20]\n",
    "subdata = subdata[np.sum((subdata.X > 0), axis=1) > 100]\n",
    "subdata = subdata[:, subdata.X.sum(axis=0) > 20]\n",
    "\n",
    "subdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f82edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d460a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.sum(subdata.X > 0, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ec6e7",
   "metadata": {},
   "source": [
    "## Save Barcodes Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save barcodes of the subsetted adata \n",
    "barcodes = list(subdata.obs.index)\n",
    "\n",
    "df = pd.DataFrame({\"barcode\": barcodes})\n",
    "# By default pandas writes a header and index; you can disable both:\n",
    "df.to_csv(\n",
    "    \"scf_testdata/10k_PBMCs_merged_barcodes.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,      # don't write row numbers\n",
    "    header=False      # omit the column name if you just want raw barcodes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b296a566",
   "metadata": {},
   "source": [
    "## Save Peaks Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Peaks / Bins \n",
    "subdata.var.to_csv(\n",
    "    \"scf_testdata/10k_PBMCs_merged_peaks.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,      # don't write row numbers\n",
    "    header=False      # omit the column name if you just want raw barcodes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc9b5b",
   "metadata": {},
   "source": [
    "## Save Subset Fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d006dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fragments \n",
    "fragments_path = \"/mnt/workspace2/jdetlef/experimental/inspect_testdata/10k_PBMC-selection/fragments\"\n",
    "PBMC_v1 = '10k_pbmc_ATACv1p1_nextgem_Chromium_X_fragments.tsv.gz'\n",
    "PBMC_v2 = '10k_pbmc_ATACv2_nextgem_Chromium_Controller_fragments.tsv.gz'\n",
    "\n",
    "PBMC_v1 = os.path.join(fragments_path, PBMC_v1)\n",
    "PBMC_v2 = os.path.join(fragments_path, PBMC_v2)\n",
    "\n",
    "PBMC_v1_fragments = pd.read_csv(PBMC_v1, header=header, delimiter=delimiter, comment='#')\n",
    "PBMC_v2_fragments = pd.read_csv(PBMC_v2, header=header, delimiter=delimiter, comment='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e77b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Batch information \n",
    "def batch_fragments(fragments, batch):\n",
    "    \n",
    "    batched_barcodes = []\n",
    "\n",
    "    for row in fragments.iterrows():\n",
    "        raw_barcode = row[1][3].split('-')[0]\n",
    "        batched_barcodes.append(raw_barcode + '-' + str(batch))\n",
    "\n",
    "    fragments[3] = batched_barcodes\n",
    "\n",
    "    return fragments\n",
    "\n",
    "print('processing')\n",
    "PBMC_v1_fragments = batch_fragments(PBMC_v1_fragments, 1)\n",
    "print('DONE:1')\n",
    "PBMC_v2_fragments = batch_fragments(PBMC_v2_fragments, 0)\n",
    "print('DONE:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453051f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate fragments\n",
    "fragments = pd.concat([PBMC_v1_fragments, PBMC_v2_fragments], ignore_index=True)\n",
    "fragments.to_csv(os.path.join(fragments_path, \"combined_fragments.tsv\"), sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "#fragments = pd.read_csv(os.path.join(fragments_path, \"combined_fragments.tsv\"), sep=\"\\t\", header=None)\n",
    "#fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bcc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroms_to_keep = ['chr1']\n",
    "# Subset fragments\n",
    "sub_fragments = fragments[fragments[3].isin(barcodes)]\n",
    "sub_fragments = sub_fragments[sub_fragments[0].isin(chroms_to_keep)]\n",
    "sub_fragments.reset_index(inplace=True)\n",
    "sub_fragments.pop('index')\n",
    "sub_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e65f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fragments_bc = 200\n",
    "\n",
    "sampling = []\n",
    "for bc, grp in sub_fragments.groupby(3):\n",
    "    selection = grp.iloc[np.random.choice(np.arange(len(grp)), n_fragments_bc)]\n",
    "    sampling.append(selection)\n",
    "    \n",
    "sub_fragments = pd.concat(sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f01054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fragments\n",
    "sub_fragments.to_csv(\n",
    "    os.path.join(fragments_path, \"subdata_fragments.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    index=False,      # don't write row numbers\n",
    "    header=False      # omit the column name if you just want raw barcodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38390319",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b64b305",
   "metadata": {},
   "source": [
    "## Save Subset BAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8214ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bamfile = '/mnt/workspace2/jdetlef/experimental/inspect_testdata/10k_PBMC-selection/bamfiles/10k_pbmc_sorted.bam'\n",
    "subdata_bam = '/mnt/workspace2/jdetlef/experimental/inspect_testdata/scf_testdata/10k_PBMCs_merged_bam.bam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_in = bamfile\n",
    "bam_out = subdata_bam\n",
    "read_tag = \"CB\"\n",
    "pysam_threads = 4,\n",
    "overwrite = False\n",
    "\n",
    "chroms_to_keep= ['chr1']\n",
    "\n",
    "# Subset merged bam from above based on the subsetted adata\n",
    "\n",
    "# check then load modules\n",
    "utils.checker.check_module(\"tqdm\")\n",
    "if utils.jupyter._is_notebook() is True:\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "utils.checker.check_module(\"pysam\")\n",
    "\n",
    "# Create output dir if needed\n",
    "utils.io.create_dir(bam_out)\n",
    "\n",
    "# Open files\n",
    "bam_in_obj = tools.bam.open_bam(bam_in, mode=\"rb\", verbosity=0)\n",
    "bam_out_obj = tools.bam.open_bam(bam_out, mode=\"wb\",verbosity=0, template=bam_in_obj)\n",
    "\n",
    "barcodes = set(barcodes)\n",
    "\n",
    "# Update progress based on total number of reads\n",
    "total = tools.bam.get_bam_reads(bam_in_obj)\n",
    "print(' ', end='', flush=True)  # hack for making progress bars work in notebooks; https://github.com/tqdm/tqdm/issues/485#issuecomment-473338308\n",
    "pbar_reading = tqdm(total=total, desc=\"Reading... \", unit=\"reads\")\n",
    "pbar_writing = tqdm(total=total, desc=\"% written from input\", unit=\"reads\")\n",
    "step = int(total / 10000)  # 10000 total updates\n",
    "\n",
    "# Iterate over reads\n",
    "writing_i = 0\n",
    "reading_i = 0\n",
    "written = 0\n",
    "    \n",
    "for chrom in chroms_to_keep:\n",
    "    for read in bam_in_obj.fetch(chrom):\n",
    "        if read.has_tag(read_tag) and read.get_tag(read_tag) in barcodes:\n",
    "            bam_out_obj.write(read)\n",
    "            written += 1\n",
    "            writing_i += 1\n",
    "            if writing_i == step:\n",
    "                pbar_writing.update(step)\n",
    "                pbar_writing.refresh()\n",
    "                writing_i = 0\n",
    "\n",
    "        reading_i += 1\n",
    "\n",
    "        # Update step manually - there is an overhead to update per read with hundreds of million reads\n",
    "        if reading_i == step:\n",
    "            pbar_reading.update(step)\n",
    "            pbar_reading.refresh()\n",
    "            reading_i = 0\n",
    "\n",
    "    \n",
    "# close progressbars\n",
    "pbar_reading.close()\n",
    "pbar_writing.close()\n",
    "\n",
    "# Close bamfiles\n",
    "bam_in_obj.close()\n",
    "bam_out_obj.close()\n",
    "#logger.info(f\"Wrote {written} reads to output bam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f80ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Bam before processing\n",
    "subdata_bam = '/mnt/workspace2/jdetlef/experimental/inspect_testdata/scf_testdata/10k_PBMCs_merged_sorted.bam'\n",
    "read_tag = \"CB\"\n",
    "pysam_threads = 4,\n",
    "overwrite = False\n",
    "n_reads = 250\n",
    "\n",
    "chroms_to_keep= ['chr1']\n",
    "\n",
    "# Subset bam to limit reads per barcode\n",
    "bam_in = subdata_bam\n",
    "directory = os.path.split(bam_in)[0]\n",
    "bam_out = os.path.join(directory, '10k_PBMCs_sampled_bam.bam')\n",
    "\n",
    "# Open files\n",
    "bam_in_obj = tools.bam.open_bam(bam_in, mode=\"rb\", verbosity=0)\n",
    "bam_out_obj = tools.bam.open_bam(bam_out, mode=\"wb\",verbosity=0, template=bam_in_obj)\n",
    "\n",
    "barcodes = set(barcodes)\n",
    "\n",
    "# Update progress based on total number of reads\n",
    "total = tools.bam.get_bam_reads(bam_in_obj)\n",
    "print(' ', end='', flush=True)  # hack for making progress bars work in notebooks; https://github.com/tqdm/tqdm/issues/485#issuecomment-473338308\n",
    "pbar_reading = tqdm(total=total, desc=\"Reading... \", unit=\"reads\")\n",
    "pbar_writing = tqdm(total=total, desc=\"% written from input\", unit=\"reads\")\n",
    "step = int(total / 10000)  # 10000 total updates\n",
    "\n",
    "# Iterate over reads\n",
    "writing_i = 0\n",
    "reading_i = 0\n",
    "written = 0\n",
    "\n",
    "read_dict = {}\n",
    "for read in bam_in_obj.fetch(chrom):\n",
    "    read_bc = read.get_tag(read_tag)\n",
    "    if read_bc not in read_dict:\n",
    "        read_dict[read_bc] = 1\n",
    "    else:\n",
    "        read_dict[read_bc] += 1\n",
    "    if read_dict[read_bc] <= n_reads:\n",
    "        bam_out_obj.write(read)\n",
    "        written += 1\n",
    "        writing_i += 1\n",
    "        if writing_i == step:\n",
    "            pbar_writing.update(step)\n",
    "            pbar_writing.refresh()\n",
    "            writing_i = 0\n",
    "\n",
    "    reading_i += 1\n",
    "\n",
    "    # Update step manually - there is an overhead to update per read with hundreds of million reads\n",
    "    if reading_i == step:\n",
    "        pbar_reading.update(step)\n",
    "        pbar_reading.refresh()\n",
    "        reading_i = 0\n",
    "\n",
    "    \n",
    "# close progressbars\n",
    "pbar_reading.close()\n",
    "pbar_writing.close()\n",
    "\n",
    "# Close bamfiles\n",
    "bam_in_obj.close()\n",
    "bam_out_obj.close()\n",
    "#logger.info(f\"Wrote {written} reads to output bam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a50070",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata.var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb9943",
   "metadata": {},
   "source": [
    "## Save subdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d014498",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata.obs.rename(columns={'batch':'sample'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0877a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata_barcodes = list(subdata.obs.index)\n",
    "samples = []\n",
    "\n",
    "for barcode in subdata_barcodes:\n",
    "    samples.append(barcode.split('-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151eaf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata.write(\"scf_testdata/10k_PBMCs_merged_adata.h5ad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sctoolbox",
   "language": "python",
   "name": "sctoolbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "sc_framework": {
   "version": "0.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
